# scripts/download_datasets_aws.py - AWS í™˜ê²½ ìµœì í™” ë²„ì „
"""
AWS CodeEditor í™˜ê²½ì„ ìœ„í•œ SALAMI ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
YouTube ë‹¤ìš´ë¡œë“œ ë¬¸ì œ í•´ê²° ë° ëŒ€ì•ˆ ì œê³µ
"""

import os
import sys
import requests
import zipfile
import csv
import subprocess
from pathlib import Path
from tqdm import tqdm
import pandas as pd
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
import argparse
import time
import logging
from typing import Dict, List, Tuple, Optional
import random

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class AWSOptimizedDownloader:
    """AWS í™˜ê²½ì— ìµœì í™”ëœ ë‹¤ìš´ë¡œë”"""
    
    def __init__(self, base_dir: str = "./datasets"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(exist_ok=True)
        
        self.salami_dir = self.base_dir / "salami-data-public"
        self.audio_dir = self.base_dir / "audio"
        self.audio_dir.mkdir(exist_ok=True)
        
        # User agents ë¦¬ìŠ¤íŠ¸ (ì°¨ë‹¨ ìš°íšŒ)
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        ]
        
        # ì‹¤íŒ¨í•œ ë¹„ë””ì˜¤ ì¶”ì 
        self.failed_videos = set()
        self.success_count = 0
        
    def download_file(self, url: str, dest_path: Path, desc: str = "Downloading") -> bool:
        """ì•ˆì „í•œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        try:
            headers = {'User-Agent': random.choice(self.user_agents)}
            response = requests.get(url, stream=True, headers=headers, timeout=30)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            
            with open(dest_path, 'wb') as f:
                with tqdm(total=total_size, unit='B', unit_scale=True, desc=desc) as pbar:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            pbar.update(len(chunk))
            return True
            
        except Exception as e:
            logger.error(f"Failed to download {url}: {e}")
            return False
    
    def download_salami_annotations(self):
        """SALAMI ì–´ë…¸í…Œì´ì…˜ ë‹¤ìš´ë¡œë“œ"""
        logger.info("Downloading SALAMI annotations...")
        
        salami_url = "https://github.com/DDMAL/salami-data-public/archive/refs/heads/master.zip"
        zip_path = self.base_dir / "salami.zip"
        
        if not self.salami_dir.exists():
            if self.download_file(salami_url, zip_path, "SALAMI annotations"):
                logger.info("Extracting SALAMI data...")
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    zip_ref.extractall(self.base_dir)
                
                extracted_dir = self.base_dir / "salami-data-public-master"
                if extracted_dir.exists():
                    extracted_dir.rename(self.salami_dir)
                
                zip_path.unlink()
                logger.info(f"âœ“ SALAMI data downloaded to {self.salami_dir}")
            else:
                logger.error("âœ— Failed to download SALAMI data")
                return False
        else:
            logger.info(f"âœ“ SALAMI data already exists at {self.salami_dir}")
            
        return True
    
    def download_youtube_pairings(self) -> Path:
        """YouTube ë§¤ì¹­ ì •ë³´ ë‹¤ìš´ë¡œë“œ"""
        logger.info("Downloading YouTube pairings...")
        
        pairings_url = "https://raw.githubusercontent.com/jblsmith/matching-salami/master/salami_youtube_pairings.csv"
        pairings_path = self.base_dir / "salami_youtube_pairings.csv"
        
        if not pairings_path.exists():
            if self.download_file(pairings_url, pairings_path, "YouTube pairings"):
                logger.info("âœ“ YouTube pairings downloaded")
            else:
                logger.error("âœ— Failed to download YouTube pairings")
                return None
        else:
            logger.info("âœ“ YouTube pairings already exist")
            
        return pairings_path
    
    def install_yt_dlp(self):
        """yt-dlp ì„¤ì¹˜ ë° ì—…ë°ì´íŠ¸"""
        try:
            # ìµœì‹  ë²„ì „ ì„¤ì¹˜
            subprocess.run([sys.executable, "-m", "pip", "install", "--upgrade", "yt-dlp"], 
                         check=True, capture_output=True)
            logger.info("âœ“ yt-dlp updated to latest version")
            return True
        except Exception as e:
            logger.error(f"âœ— Failed to install/update yt-dlp: {e}")
            return False
    
    def test_youtube_access(self, test_video_id: str = "dQw4w9WgXcQ") -> bool:
        """YouTube ì ‘ê·¼ í…ŒìŠ¤íŠ¸"""
        try:
            cmd = [
                "yt-dlp",
                "--quiet",
                "--no-warnings",
                "--print", "title",
                f"https://www.youtube.com/watch?v={test_video_id}"
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                logger.info("âœ“ YouTube access test passed")
                return True
            else:
                logger.warning(f"YouTube access test failed: {result.stderr}")
                return False
                
        except Exception as e:
            logger.warning(f"YouTube access test error: {e}")
            return False
    
    def download_youtube_audio_robust(self, youtube_id: str, output_path: Path, retries: int = 3) -> bool:
        """ê°•í™”ëœ YouTube ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ"""
        if output_path.exists():
            return True
            
        if youtube_id in self.failed_videos:
            return False
            
        for attempt in range(retries):
            try:
                # ë‹¤ì–‘í•œ ì„¤ì •ìœ¼ë¡œ ì‹œë„
                configs = [
                    # ê¸°ë³¸ ì„¤ì •
                    {
                        "format": "bestaudio[ext=m4a]/bestaudio/best",
                        "user_agent": random.choice(self.user_agents)
                    },
                    # ë” ê´€ëŒ€í•œ ì„¤ì •
                    {
                        "format": "worst[ext=m4a]/worst",
                        "user_agent": random.choice(self.user_agents),
                        "no_check_certificate": True
                    },
                    # ë§¤ìš° ê´€ëŒ€í•œ ì„¤ì •
                    {
                        "format": "worst",
                        "user_agent": random.choice(self.user_agents),
                        "no_check_certificate": True,
                        "prefer_insecure": True
                    }
                ]
                
                config = configs[min(attempt, len(configs) - 1)]
                
                cmd = [
                    "yt-dlp",
                    "-x",
                    "--audio-format", "mp3",
                    "--audio-quality", "5",  # ì¤‘ê°„ í’ˆì§ˆ
                    "-o", str(output_path),
                    "--no-playlist",
                    "--quiet",
                    "--no-warnings",
                    "--user-agent", config["user_agent"],
                    "--format", config["format"]
                ]
                
                if config.get("no_check_certificate"):
                    cmd.append("--no-check-certificate")
                if config.get("prefer_insecure"):
                    cmd.append("--prefer-insecure")
                
                cmd.append(f"https://www.youtube.com/watch?v={youtube_id}")
                
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
                
                if result.returncode == 0 and output_path.exists():
                    self.success_count += 1
                    return True
                    
                # íŠ¹ì • ì˜¤ë¥˜ ë©”ì‹œì§€ í™•ì¸
                error_msg = result.stderr.lower()
                if any(keyword in error_msg for keyword in ["unavailable", "private", "deleted", "blocked"]):
                    logger.warning(f"Video {youtube_id} permanently unavailable")
                    self.failed_videos.add(youtube_id)
                    return False
                    
            except subprocess.TimeoutExpired:
                logger.warning(f"Timeout downloading {youtube_id}, attempt {attempt + 1}")
            except Exception as e:
                logger.warning(f"Error downloading {youtube_id}: {e}")
                
            if attempt < retries - 1:
                wait_time = (2 ** attempt) + random.uniform(1, 3)
                time.sleep(wait_time)
                
        self.failed_videos.add(youtube_id)
        return False
    
    def parse_youtube_pairings(self, pairings_path: Path) -> List[Dict]:
        """YouTube í˜ì–´ë§ íŒŒì‹±"""
        logger.info("Parsing YouTube pairings...")
        
        df = pd.read_csv(pairings_path)
        
        required_cols = ['salami_id', 'youtube_id']
        if not all(col in df.columns for col in required_cols):
            logger.error(f"Missing required columns. Found: {df.columns.tolist()}")
            return []
        
        df = df[df['youtube_id'].notna() & (df['youtube_id'] != '')]
        
        download_list = []
        for _, row in df.iterrows():
            salami_id = str(row['salami_id'])
            youtube_id = str(row['youtube_id'])
            
            download_list.append({
                'salami_id': salami_id,
                'youtube_id': youtube_id,
                'filename': f"{salami_id}.mp3"
            })
            
        logger.info(f"Found {len(download_list)} valid YouTube matches")
        return download_list
    
    def download_youtube_batch_aws(self, download_list: List[Dict], max_files: Optional[int] = None):
        """AWS í™˜ê²½ì— ìµœì í™”ëœ YouTube ë°°ì¹˜ ë‹¤ìš´ë¡œë“œ"""
        if max_files:
            download_list = download_list[:max_files]
            
        logger.info(f"Starting AWS-optimized download of {len(download_list)} audio files...")
        
        # YouTube ì ‘ê·¼ í…ŒìŠ¤íŠ¸
        if not self.test_youtube_access():
            logger.warning("YouTube access test failed - downloads may fail")
            
        self.success_count = 0
        failed_count = 0
        
        # ìˆœì°¨ ë‹¤ìš´ë¡œë“œ (AWS í™˜ê²½ì—ì„œ ë” ì•ˆì •ì )
        with tqdm(total=len(download_list), desc="Downloading audio") as pbar:
            for i, item in enumerate(download_list):
                try:
                    success = self.download_youtube_audio_robust(
                        item['youtube_id'],
                        self.audio_dir / item['filename']
                    )
                    
                    if not success:
                        failed_count += 1
                        
                    # ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸
                    pbar.set_postfix({
                        'success': self.success_count,
                        'failed': failed_count,
                        'rate': f"{(self.success_count / (i + 1) * 100):.1f}%"
                    })
                    
                    # AWS í™˜ê²½ì—ì„œ rate limiting
                    if i % 10 == 0:
                        time.sleep(2)
                        
                except KeyboardInterrupt:
                    logger.info("Download interrupted by user")
                    break
                except Exception as e:
                    logger.error(f"Unexpected error with {item['salami_id']}: {e}")
                    failed_count += 1
                    
                finally:
                    pbar.update(1)
        
        # ê²°ê³¼ ìš”ì•½
        total_attempted = self.success_count + failed_count
        success_rate = (self.success_count / total_attempted * 100) if total_attempted > 0 else 0
        
        logger.info(f"\n{'='*50}")
        logger.info(f"AWS Download Summary:")
        logger.info(f"  Successful: {self.success_count}")
        logger.info(f"  Failed: {failed_count}")
        logger.info(f"  Success rate: {success_rate:.1f}%")
        logger.info(f"{'='*50}\n")
        
        # ì‹¤íŒ¨í•œ ë¹„ë””ì˜¤ ëª©ë¡ ì €ì¥
        if self.failed_videos:
            failed_path = self.base_dir / "failed_videos_aws.txt"
            with open(failed_path, 'w') as f:
                f.write('\n'.join(self.failed_videos))
            logger.info(f"Failed video IDs saved to: {failed_path}")
        
        return self.success_count, failed_count
    
    def create_alternative_dataset_info(self):
        """ëŒ€ì•ˆ ë°ì´í„°ì…‹ ì •ë³´ ìƒì„±"""
        logger.info("Creating alternative dataset information...")
        
        info = {
            'download_method': 'aws_optimized',
            'salami_dir': str(self.salami_dir),
            'audio_dir': str(self.audio_dir),
            'statistics': {}
        }
        
        # SALAMI í†µê³„
        if self.salami_dir.exists():
            ann_dir = self.salami_dir / "annotations"
            if ann_dir.exists():
                info['statistics']['total_annotations'] = len(list(ann_dir.iterdir()))
                
            metadata_path = self.salami_dir / "metadata.csv"
            if metadata_path.exists():
                metadata = pd.read_csv(metadata_path)
                info['statistics']['total_songs'] = len(metadata)
        
        # ì‹¤ì œ ë‹¤ìš´ë¡œë“œëœ ì˜¤ë””ì˜¤
        audio_files = list(self.audio_dir.glob("*.mp3"))
        info['statistics']['downloaded_audio'] = len(audio_files)
        info['statistics']['success_count'] = self.success_count
        info['statistics']['failed_count'] = len(self.failed_videos)
        
        # ì •ë³´ ì €ì¥
        info_path = self.base_dir / "dataset_info_aws.json"
        with open(info_path, 'w') as f:
            json.dump(info, f, indent=2)
            
        logger.info(f"\n{'='*50}")
        logger.info("AWS DATASET INFORMATION")
        logger.info(f"{'='*50}")
        for key, value in info['statistics'].items():
            logger.info(f"  {key}: {value}")
        logger.info(f"{'='*50}\n")
        
        return info
    
    def suggest_alternatives(self):
        """ëŒ€ì•ˆ ì œì•ˆ"""
        logger.info("Suggesting alternatives for failed downloads...")
        
        alternatives = [
            "1. Manual download: Use a local machine to download and upload to AWS",
            "2. Use different AWS region: Some regions may have different access",
            "3. Use AWS EC2 with different settings: Configure proxy or VPN",
            "4. Download subset: Focus on successfully downloaded files only",
            "5. Use Internet Archive: Some tracks may be available there"
        ]
        
        logger.info("Alternative approaches:")
        for alt in alternatives:
            logger.info(f"  {alt}")
    
    def download_all_aws(self, max_files: Optional[int] = None):
        """AWS í™˜ê²½ìš© ì „ì²´ ë‹¤ìš´ë¡œë“œ í”„ë¡œì„¸ìŠ¤"""
        logger.info("Starting AWS-optimized SALAMI download process...")
        
        # 1. SALAMI ì–´ë…¸í…Œì´ì…˜ ë‹¤ìš´ë¡œë“œ
        if not self.download_salami_annotations():
            return
        
        # 2. yt-dlp ì—…ë°ì´íŠ¸
        if not self.install_yt_dlp():
            logger.warning("Could not update yt-dlp, proceeding with existing version")
        
        # 3. YouTube pairings ë‹¤ìš´ë¡œë“œ
        pairings_path = self.download_youtube_pairings()
        if not pairings_path:
            return
        
        # 4. YouTube ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ (AWS ìµœì í™”)
        download_list = self.parse_youtube_pairings(pairings_path)
        if download_list:
            self.download_youtube_batch_aws(download_list, max_files)
        
        # 5. ë°ì´í„°ì…‹ ì •ë³´ ìƒì„±
        self.create_alternative_dataset_info()
        
        # 6. ê²°ê³¼ì— ë”°ë¥¸ ê°€ì´ë“œ
        if self.success_count > 10:
            logger.info(f"\nğŸ‰ Successfully downloaded {self.success_count} files!")
            logger.info("You can proceed with data processing:")
            logger.info(f"   python scripts/prepare_salami_data.py \\")
            logger.info(f"       --salami_root {self.salami_dir} \\")
            logger.info(f"       --audio_root {self.audio_dir}")
        elif self.success_count > 0:
            logger.info(f"\nâš ï¸ Partially successful: {self.success_count} files downloaded")
            logger.info("You can still proceed with limited data:")
            logger.info("   python scripts/prepare_salami_data.py --min_duration 5 --max_duration 300")
        else:
            logger.warning("\nâŒ No files successfully downloaded")
            self.suggest_alternatives()


def main():
    parser = argparse.ArgumentParser(
        description="AWS-optimized SALAMI dataset downloader",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
AWS Environment Examples:
  # Download small batch for testing
  python scripts/download_datasets_aws.py --max-files 20
  
  # Download with more retries
  python scripts/download_datasets_aws.py --max-files 50
  
  # Download everything (may take very long)
  python scripts/download_datasets_aws.py
        """
    )
    
    parser.add_argument('--base-dir', type=str, default='./datasets',
                        help='Base directory for datasets (default: ./datasets)')
    parser.add_argument('--max-files', type=int, default=100,
                        help='Maximum number of audio files to download (default: 100)')
    
    args = parser.parse_args()
    
    # AWS í™˜ê²½ ê°ì§€
    is_aws = any([
        os.getenv('AWS_EXECUTION_ENV'),
        os.getenv('AWS_LAMBDA_FUNCTION_NAME'),
        'amazonaws.com' in os.getenv('HOSTNAME', ''),
        os.path.exists('/opt/aws')
    ])
    
    if is_aws:
        logger.info("ğŸŒ©ï¸ AWS environment detected - using optimized settings")
    else:
        logger.info("ğŸ’» Local environment detected - using standard settings")
    
    # ë‹¤ìš´ë¡œë” ìƒì„± ë° ì‹¤í–‰
    downloader = AWSOptimizedDownloader(args.base_dir)
    downloader.download_all_aws(max_files=args.max_files)


if __name__ == "__main__":
    main()